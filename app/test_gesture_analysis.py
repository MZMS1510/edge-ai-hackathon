#!/usr/bin/env python3
"""
Teste EspecÃ­fico de AnÃ¡lise de Gestos
Verifica se a anÃ¡lise de gestos estÃ¡ funcionando corretamente
"""

import cv2
import mediapipe as mp
import numpy as np
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'core'))

from analysis import CommunicationAnalyzer

def test_gesture_analysis():
    """Testa anÃ¡lise de gestos em tempo real"""
    print("ðŸŽ¯ Teste de AnÃ¡lise de Gestos")
    print("=" * 50)
    
    # Inicializar MediaPipe
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=2,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    
    # Inicializar analisador
    analyzer = CommunicationAnalyzer()
    
    print("ðŸ“Š ConfiguraÃ§Ã£o de Gestos:")
    print(f"  movement_threshold_low: {analyzer.config['gesture']['movement_threshold_low']:.3f}")
    print(f"  movement_threshold_high: {analyzer.config['gesture']['movement_threshold_high']:.3f}")
    print(f"  base_score_no_hands: {analyzer.config['gesture']['base_score_no_hands']}")
    
    # Abrir cÃ¢mera
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    if not cap.isOpened():
        print("âŒ Erro: NÃ£o foi possÃ­vel abrir a cÃ¢mera")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    print("\nðŸŽ¥ CÃ¢mera aberta com sucesso!")
    print("ðŸ“‹ InstruÃ§Ãµes:")
    print("  - Fique parado (sem gestos) e observe o score")
    print("  - FaÃ§a gestos com as mÃ£os e observe o score aumentar")
    print("  - Use uma mÃ£o ou duas mÃ£os")
    print("  - Pressione 'q' para sair")
    print("  - Pressione 'r' para resetar scores")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # Converter para RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Processar com MediaPipe
        hands_results = hands.process(rgb_frame)
        
        # Analisar gestos
        gesture_score = analyzer.analyze_gestures(hands_results)
        
        # Calcular movimento real para debug
        movement_info = "Sem mÃ£os detectadas"
        if hands_results.multi_hand_landmarks:
            total_movement = 0
            hand_count = len(hands_results.multi_hand_landmarks)
            
            for hand_landmarks in hands_results.multi_hand_landmarks:
                wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]
                thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]
                index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
                middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
                
                hand_movement = (abs(wrist.y - thumb_tip.y) + 
                               abs(wrist.y - index_tip.y) + 
                               abs(wrist.y - middle_tip.y)) / 3
                total_movement += hand_movement
            
            avg_movement = total_movement / hand_count if hand_count > 0 else 0
            movement_info = f"MÃ£os: {hand_count}, Movimento: {avg_movement:.3f}"
        
        # Desenhar landmarks
        annotated_frame = frame.copy()
        if hands_results.multi_hand_landmarks:
            for hand_landmarks in hands_results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    annotated_frame, 
                    hand_landmarks, 
                    mp_hands.HAND_CONNECTIONS
                )
        
        # Mostrar informaÃ§Ãµes na tela
        cv2.putText(annotated_frame, f"Score Gestos: {gesture_score:.1f}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(annotated_frame, movement_info, 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Mostrar thresholds
        cv2.putText(annotated_frame, f"Low: {analyzer.config['gesture']['movement_threshold_low']:.3f}", 
                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
        cv2.putText(annotated_frame, f"High: {analyzer.config['gesture']['movement_threshold_high']:.3f}", 
                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
        
        # InstruÃ§Ãµes
        cv2.putText(annotated_frame, "Fique parado ou faÃ§a gestos", 
                   (10, annotated_frame.shape[0] - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(annotated_frame, "Pressione 'q' para sair, 'r' para resetar", 
                   (10, annotated_frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.imshow('Teste de AnÃ¡lise de Gestos', annotated_frame)
        
        # Controles
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('r'):
            analyzer.reset_scores()
            print("ðŸ”„ Scores resetados!")
    
    cap.release()
    cv2.destroyAllWindows()
    
    print("\nðŸ“Š Resumo do Teste:")
    print("âœ… AnÃ¡lise de gestos corrigida!")
    print("ðŸŽ¯ Agora o sistema deve:")
    print("   - Dar score baixo quando vocÃª estÃ¡ parado")
    print("   - Dar score alto quando vocÃª faz gestos")
    print("   - Diferenciar entre pouco e muito movimento")

if __name__ == "__main__":
    test_gesture_analysis()
